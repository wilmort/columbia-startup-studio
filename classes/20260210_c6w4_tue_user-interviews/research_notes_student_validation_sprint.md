# Research Notes: Student Validation Sprint — Practical Guidance for Week 4

**Date:** 2026-02-09
**Researcher:** Claude (Opus 4.6)
**Purpose:** Raw research notes on practical student team validation — interviews, logistics, synthesis, and next steps
**Complements:** `research_notes_pre_build_validation.md` (technique taxonomy and frameworks)

---

## TABLE OF CONTENTS

1. [How Much Validation in 1-2 Weeks?](#1-how-much-validation-in-1-2-weeks)
2. [Practical Validation Sprint Plan](#2-practical-validation-sprint-plan)
3. [Interview Logistics for Students](#3-interview-logistics-for-students)
4. [Interview Scripts and Question Frameworks](#4-interview-scripts-and-question-frameworks)
5. [Synthesizing Interview Findings](#5-synthesizing-interview-findings)
6. [Interviews to Next Steps Pipeline](#6-interviews-to-next-steps-pipeline)
7. [Academic Research on Student Startup Validation](#7-academic-research-on-student-startup-validation)

---

## 1. How Much Validation in 1-2 Weeks?

### What's Realistic for Student Teams

**The benchmark: Stanford's Lean LaunchPad**

Stanford's ENGR 245 (Lean LaunchPad), taught by Steve Blank, explicitly requires **10-15 customer interviews per week per team**. In the 2024 Spring quarter, 8 teams collectively spoke to **919 potential customers, beneficiaries, and regulators** over the course of the quarter. Students spent **15-20 hours per week** on the class — about double a normal course.
- Source: [Steve Blank, "Lean LaunchPad at Stanford 2024"](https://steveblank.com/2024/06/27/lean-launchpad-stanford-2024-8-teams-in-8-companies-out/)
- Source: [Stanford ENGR 245 course page](https://leanlaunchpad.stanford.edu/)

In the 2025 iteration, the eight teams spoke to **935 potential customers, beneficiaries, and regulators**.
- Source: [Steve Blank, "Lean LaunchPad at Stanford 2025"](https://steveblank.com/2025/06/24/lean-launchpad-at-stanford-2025/)

**NSF I-Corps (the gold standard for structured customer discovery):**
Teams must complete a minimum of **100 face-to-face interviews** during a 7-week program. That breaks down to roughly **14-15 interviews per week** — a very aggressive pace, but this is the program's entire focus.
- Source: [NSF I-Corps FAQ](https://www.nsf.gov/pubs/2017/nsf17083/nsf17083.jsp)
- Source: [NSF I-Corps About](https://www.nsf.gov/funding/initiatives/i-corps/about-i-corps)

**For Columbia Startup Studio, what's realistic in 1-2 weeks:**

Given that:
- Teams of 4-5 students
- This is NOT their only class (unlike I-Corps which is a dedicated program)
- They need time to prepare, recruit, conduct, and synthesize
- They're learning interviewing skills for the first time

A realistic target: **8-12 interviews per team over 2 weeks** (roughly 2-3 per team member). This is achievable if they start recruiting immediately.

### How Many Interviews Is "Enough"?

**The research shows a wide range of recommendations:**

| Source | Recommended # | Context |
|--------|--------------|---------|
| Customer Dev Labs (Justin Wilcox) | **Batches of 5**, stop when 3/5 describe the same problems | Early-stage problem discovery |
| Nielsen Norman Group | **5-6 minimum**, analyze as you go, add more until saturation | UX interviews — scoped research |
| NNGroup (broader exploration) | **12-20** for thematic saturation with diverse populations | Exploratory interview studies |
| Growth Ramp | **20-30** for deeper understanding | Comprehensive customer discovery |
| NSF I-Corps | **100 minimum** over 7 weeks | Full-program customer development |
| Stanford Lean LaunchPad | **10-15 per week**, ongoing | Semester-long program |

- Source: [Customer Dev Labs, "How Many Customers?"](https://customerdevlabs.com/2017/04/10/how-many-customers-should-you-interview/)
- Source: [NNGroup, "Interview Sample Size"](https://www.nngroup.com/articles/interview-sample-size/)
- Source: [Growth Ramp, "Customer Discovery Interviews"](https://www.growthramp.io/articles/customer-discovery-interviews)

**The 3-of-5 Rule (Customer Dev Labs):**
Interview in batches of 5. After each batch, check: do at least 3 of the last 5 people describe **the same problems in the same way**? If yes, you've found your early adopters. If not, refine your target and interview another batch of 5. This is especially useful for student teams with limited time — it gives a concrete stopping condition.
- Source: [Customer Dev Labs](https://customerdevlabs.com/2017/04/10/how-many-customers-should-you-interview/)

**NNGroup on saturation:**
Studies using empirical data reached saturation within a narrow range of **9-17 interviews**, particularly with relatively homogeneous study populations and narrowly defined objectives. Mark Mason's analysis of over 2,000 PhD theses using qualitative interviews found a median sample of 31 but a standard deviation of 18.7 — meaning there's huge variance.
- Source: [NNGroup](https://www.nngroup.com/articles/interview-sample-size/)

**Bottom line for our students:** 8-12 interviews in 2 weeks is achievable and will produce actionable signal if they interview well. Quality matters exponentially more than quantity. 5 excellent Mom Test interviews beat 20 "do you like my idea?" conversations.

### Other Validation Methods Beyond Interviews (for 1-2 Weeks)

Not everything has to be interviews. Student teams can also use:

1. **Online surveys** (after initial interviews — to quantify patterns found in interviews). Free via Google Forms. Can reach 50-100+ respondents in days.
   - Source: [SurveyMonkey](https://www.surveymonkey.com/curiosity/validate-startup-idea-consumer-behavior-bridgecare-finance/)

2. **Competitive analysis / review mining** — Read 1-star reviews of competitor products on App Store, G2, Reddit. What are people complaining about? What's missing?
   - Source: [Antler Academy](https://www.antler.co/academy/startup-competitor-analysis)

3. **Social media listening** — Search Reddit, Twitter, and niche forums for discussions about the problem space. What language do people use? What solutions have they tried?
   - Source: [StartupEspresso](https://startupespresso.live/validate-startup-concept-with-social-media/)

4. **Guerrilla testing** — Quick 5-10 minute intercept interviews in public spaces (campus, coffee shops, subway). Lower quality but higher volume.
   - Source: [UX Mastery](https://uxmastery.com/guerrilla-ux-research/)

---

## 2. Practical Validation Sprint Plan

### The 5-Day Design Sprint Model (Jake Knapp / Google Ventures)

The original Design Sprint from Jake Knapp's book *Sprint*:
- **Monday:** Map the problem, select focus area, identify target customers
- **Tuesday:** Sketch solutions (Lightning Demos — each team member reviews inspiring solutions)
- **Wednesday:** Decide which solutions to pursue
- **Thursday:** Build a realistic prototype
- **Friday:** Test with 5 target customers in 1:1 interviews

- Source: [The Official 5-Day Design Sprint (Coda)](https://coda.io/@jazer/design-sprint)
- Source: [Marty Cagan / SVPG, "Discovery Sprints"](https://www.svpg.com/discovery-sprints/)

### The Foundation Sprint (Jake Knapp & John Zeratsky, 2024)

A compressed 2-day version for early-stage startups:
- **Phase 1:** Establish the basics (problem, target customer, core value proposition)
- **Phase 2:** Differentiation — use a 2x2 framework to find what makes the approach unique. "Differentiation is the #1 predictor of startup success."
- **Phase 3:** Approach — use "magic lenses" (7 perspectives) to evaluate which product ideas to pursue
- Key insight: "Most startups fail not because they can't build, but because they build the wrong thing."

- Source: [Lenny's Newsletter, "The Foundation Sprint"](https://www.lennysnewsletter.com/p/the-foundation-sprint-jake-knapp-and-john-zeratsky)
- Source: [Lenny's Newsletter, "Introducing the Foundation Sprint"](https://www.lennysnewsletter.com/p/introducing-the-foundation-sprint)

### Recommended Day-by-Day Plan for Student Teams (Week 4)

This plan assumes teams formed Week 3 and generated ideas. Now they need to validate.

**Assumptions:**
- Team of 4-5 students
- ~5-8 hours per student available this week for this work (outside class time)
- Two class sessions (Tuesday c6w4, Thursday c7w4)
- Target: 8-12 interviews total by end of Week 4

---

#### DAYS 1-2 (Monday-Tuesday, Feb 9-10)

**Goal: Prepare to interview and start recruiting**

1. **Define your riskiest assumption** (30 min, full team)
   - What's the single biggest bet you're making? Usually: "People have this problem and it's painful enough to motivate action."
   - Write it down as a testable hypothesis: "We believe [target user] struggles with [problem] because [reason], and would [change behavior / pay] for a solution."

2. **Create your interview script** (45 min, 2 people)
   - Follow Justin Wilcox's 5-question framework (see Section 4 below)
   - Write your custom top 3 questions
   - Practice with each other — do at least one mock interview

3. **Begin recruiting** (ongoing, all team members)
   - Each team member commits to sourcing 3 potential interviewees
   - Post in Columbia-specific channels (see Section 3 for specific locations)
   - Reach out to personal networks with referral requests
   - Target: schedule 4-6 interviews for the next 3 days

4. **Tuesday class (c6w4):** Lecture on user interviews / concept validation. Teams practice interviewing techniques in class.

---

#### DAYS 3-4 (Wednesday-Thursday, Feb 11-12)

**Goal: Conduct first batch of interviews (5-6 interviews)**

5. **Conduct interviews in pairs** (2-3 per day)
   - One person asks questions, one takes notes
   - 15-20 minutes per interview (shorter is fine for problem discovery)
   - Record audio (with consent) for review later
   - Immediately after each interview: 5-minute debrief between the pair

6. **Quick debrief after each day** (15 min, full team)
   - Each pair shares: "The #1 thing we learned today" and "What surprised us"
   - Adjust interview questions based on what you're hearing
   - Note any emerging patterns

7. **Continue recruiting** for next batch
   - Ask current interviewees: "Who else should I talk to?" (snowball recruiting)
   - This single question is the most powerful recruiting tool you have

8. **Thursday class (c7w4):** Define audience based on interview learnings. Teams share initial findings.

---

#### DAYS 5-7 (Friday-Sunday, Feb 13-15)

**Goal: Second batch of interviews (4-6 more) + initial synthesis**

9. **Conduct remaining interviews** (4-6 more across the weekend)
   - Can be remote (Zoom) or in person
   - If patterns are emerging, adjust questions to probe deeper on those areas
   - If patterns are NOT emerging, you may need to narrow your target audience

10. **Synthesis session** (60-90 min, full team, Sunday evening)
    - Use the affinity mapping approach (see Section 5)
    - Identify top 3 patterns / problems that emerged
    - Rate each: frequency, severity, current solutions, willingness to change
    - Answer: "Based on interviews, is our original hypothesis confirmed, modified, or invalidated?"

11. **Write a 1-page validation summary** (30 min, one person)
    - What we hypothesized
    - Who we talked to (# of interviews, demographics)
    - What we learned (top 3 findings)
    - Decision: proceed, pivot, or dig deeper
    - What this means for our vaporware site (Week 5)

---

#### INTO WEEK 5 (Feb 17-19)

**Goal: Translate interview insights into vaporware site**

12. **Use interview language** for your landing page headlines and value proposition
    - The words your interviewees used to describe their problems are your copy
    - The specific pain points they mentioned are your feature bullets

13. **Build vaporware site** (Tuesday c8w5) using interview findings
14. **Wizard of Oz testing** (Thursday c9w5) — informed by what you learned about how users expect solutions to work

---

## 3. Interview Logistics for Students

### Where to Find Interviewees

**On-campus (Columbia-specific):**
- **Other Columbia students** — the most accessible population; useful if your target audience includes students, young professionals, or NYC residents
- **Student organizations** — email listservs, Slack channels, GroupMe chats related to your problem space
- **Columbia classroom announcements** — ask professors if you can make a 30-second pitch at the start of other classes
- **Campus common areas** — Butler Library, Lerner Hall, campus cafes — guerrilla intercept interviews
- **Columbia Facebook/social groups** — Class of 20XX groups, department groups, interest groups

**Off-campus (NYC):**
- **Coffee shops and co-working spaces** — approach people who are sitting (not walking; sitting people are more receptive because they'd have to stand up to walk away)
  - Source: [Smashing Magazine, "The Art of the Intercept"](https://www.smashingmagazine.com/2016/02/the-art-of-the-intercept/)
- **NYC Meetups** — attend events related to your problem space; interview attendees
- **Public transit** — Columbia students can approach people during commutes (brief 5-minute interviews)
- **Public parks** — Morningside Park, Riverside Park, Central Park — people sitting on benches

**Online:**
- **Reddit** — Find subreddits related to your problem space. Engage first (don't just drop in asking for interviews). Contact moderators before posting. Build some karma in the community first.
  - Source: [SAGE Journals, "Using Reddit as a Source for Recruiting Participants"](https://journals.sagepub.com/doi/full/10.1177/16094069231162674)
- **Discord/Slack communities** — Fewer fake applicants than Reddit but harder to find the right groups
  - Source: [Manatal, "Recruiting on Slack & Discord"](https://www.manatal.com/blog/recruit-on-slack-and-discord)
- **LinkedIn** — Post about the problem space, reply to thought leaders, DM target users. "If people are really struggling with a problem, and you can label it, they'll talk to you."
  - Source: [Product Talk, "Finding Interview Participants"](https://www.producttalk.org/finding-interview-participants-before-customers/)
- **Twitter/X** — Search for people complaining about problems in your space
- **UserInterviews.com** — Paid platform ($50-100/participant) that handles recruiting and scheduling. Can get 5 interviews scheduled within a few days. Worth considering if budget allows.
  - Source: [UserInterviews](https://www.userinterviews.com/)

### Recruiting Messages (Templates)

**Cold outreach (short):**
> Hi [Name]! I'm a Columbia student researching how people deal with [problem area]. I'm not selling anything — just trying to understand this problem better. Would you be open to a 15-minute chat? I can work around your schedule. Happy to buy you coffee as a thank-you!

**Posting in communities:**
> Hi everyone! I'm a Columbia grad student researching [problem area] for a class project. I'm looking to have 15-minute conversations with people who [specific qualifier — e.g., "commute to work in NYC," "cook meals at home 3+ times/week," "manage personal finances"]. Not selling anything, just learning! DM me or comment below if you'd be willing to chat. Thank you!

**Asking for referrals (after an interview):**
> Thank you so much for your time — this was incredibly helpful. Do you know 1-2 other people who deal with [problem] that might be willing to chat with me? An intro would be amazing.

- Source: [Rob Fitzpatrick, The Mom Test](https://www.ricklindquist.com/notes/the-mom-test-by-rob-fitzpatrick) (framing as "research" and requesting help)

### How to Conduct Interviews

**In-person vs. Remote:**

| Factor | In-Person | Remote (Zoom/Phone) |
|--------|-----------|-------------------|
| **Body language** | Full visibility — see hesitations, excitement, discomfort | Limited (video) or none (phone) |
| **Rapport** | Easier to build trust face-to-face | Harder; needs more warm-up time |
| **Convenience** | Harder to schedule — requires same place/time | Easier — just send a link |
| **Recording** | Need consent; use phone voice memo app | Zoom recording; Otter.ai for transcription |
| **Best for** | Deeper conversations, prototype testing | Broader geographic reach, faster scheduling |

**Recommendation for students:** Do at least 3-4 in person (campus or coffee shops) and the rest remote. In-person interviews are higher quality but remote is more practical for hitting volume targets.

**Interview pair roles:**

Best practice is to conduct interviews in pairs:
- **Interviewer:** Asks questions, maintains eye contact, builds rapport, follows the conversation
- **Note-taker:** Captures key quotes, observations, body language cues. Uses pen and paper (not laptop — laptops create a barrier). Writes down direct quotes in quotation marks.
  - Source: [Teresa Torres / Product Talk, "How to Take Notes"](https://www.producttalk.org/how-to-take-notes-during-customer-research-interviews/)

**Recording and consent:**

Always tell participants if recording. New York is a one-party consent state for audio recording, but best practice is still to ask explicitly: "Would you mind if I record this for my notes? I won't share the recording with anyone outside our team."
- Source: [BlueDot, "Best Practices for Consent to Record"](https://www.bluedothq.com/blog/obtaining-consent-to-record-meetings)

**Tools:**
- Voice Memos app (iPhone) for in-person recording
- Zoom recording for remote interviews
- Otter.ai for automatic transcription ($16.99/month or free with limits)
  - Source: [Otter.ai](https://otter.ai/)
- Google Docs for shared note-taking
- Notion or Airtable for interview database

### Documentation Template (Per Interview)

```
INTERVIEW #: ___
DATE: ___
INTERVIEWER: ___
NOTE-TAKER: ___
PARTICIPANT: [Name or pseudonym]
DEMOGRAPHICS: [Age range, occupation, relevant context]
SETTING: [In-person / Zoom / Phone]
DURATION: ___ minutes

SCREENING: Does this person experience the problem? [Y/N/Partial]

KEY QUOTES (verbatim):
1. "..."
2. "..."
3. "..."

PROBLEM SEVERITY: [1-5 scale or description]
CURRENT SOLUTIONS: [What do they do today?]
MONEY SPENT ON PROBLEM: [$ or time or effort]
WILLINGNESS TO CHANGE: [High/Medium/Low — based on behavior, not words]

TOP INSIGHT: [One sentence — the most surprising or important thing from this interview]

FOLLOW-UP NEEDED? [Y/N — what specifically?]
REFERRAL GIVEN? [Y/N — who?]
```

- Adapted from: [Kromatic Customer Discovery Note Taking Template](https://www.kromatic.com/innovation-resources/customer-discovery-note-taking-template)
- Also see: [LogRocket spreadsheet template](https://blog.logrocket.com/product-management/managing-customer-discovery-interview-data-spreadsheet-template/) with downloadable Google Sheet

---

## 4. Interview Scripts and Question Frameworks

### The Mom Test Rules (Rob Fitzpatrick)

**Rule 1: Talk about their life, not your idea.**
- Bad: "Do you think it's a good idea?"
- Good: "What's the hardest part about [activity/problem area]?"

**Rule 2: Ask about specifics in the past, not hypotheticals about the future.**
- Bad: "Would you pay for a product that does X?"
- Good: "The last time you dealt with this, what did you do?"

**Rule 3: Talk less, listen more.**
- "The more you're talking, the worse you're doing."
- Target: They should be talking 70-80% of the time.

**Three types of bad data to watch for:**
1. **Compliments** — "That's so cool!" (deflect by asking "Why?")
2. **Hypothetical fluff** — "I would definitely use that" (anchor by asking "When was the last time...")
3. **Wishlists** — "It would be cool if it could also..." (dig into motivation: "Why do you want that?")

- Source: [Rob Fitzpatrick / The Mom Test, via Khanna Law summary](https://www.khanna.law/notes/the-mom-test)
- Source: [Sachin Rekhi, "A Primer on The Mom Test"](https://www.sachinrekhi.com/the-mom-test-rob-fitzpatrick)
- Source: [Rick Lindquist, "Notes from The Mom Test"](https://www.ricklindquist.com/notes/the-mom-test-by-rob-fitzpatrick)

### Justin Wilcox's 5-Question Framework (Customer Dev Labs)

This is a battle-tested script for first-time interviewers. It works for 15-20 minute problem interviews.

1. **"What's the hardest part about [doing the thing your product helps with]?"**
   - Opens with an open-ended question about their actual experience
   - Don't mention your solution — frame it around the activity/problem area

2. **"Can you tell me about the last time that happened?"**
   - Forces them into a specific story (not a generalization)
   - Stories reveal 23x more learning than yes/no answers

3. **"Why was that hard?"**
   - Uncovers the emotional dimension — frustration, fear, wasted time, wasted money
   - Gets at the real motivation behind the surface problem

4. **"What, if anything, have you done to try to solve that problem?"**
   - Critical signal: if they haven't looked for solutions, they probably won't buy yours
   - "Rule of thumb: If they haven't looked for ways of solving it already, they're not going to look for or buy yours."

5. **"What don't you love about the solutions you've tried?"**
   - Identifies gaps in existing solutions — your differentiation opportunity
   - If they love existing solutions, you have a harder battle

**Bonus questions:**
- "Is there anything else about [problem area] that I should know about?"
- "Do you know anyone else who deals with this that I could talk to?"

- Source: [Customer Dev Labs, "How I Interview Customers"](https://customerdevlabs.com/2013/11/05/how-i-interview-customers/)
- Source: [Customer Dev Labs Script Generator](https://customerdevlabs.com/script/)

### Full 15-20 Minute Interview Script for Student Teams

```
WARM-UP (2 min)
"Thanks so much for taking the time. I'm a Columbia student working on
a class project, and I'm trying to understand how people deal with
[problem area]. I'm not selling anything — I genuinely want to learn
from your experience. There are no right or wrong answers."

[If recording] "Would it be OK if I record this for my notes? I won't
share it outside our team."

CONTEXT / SCREENING (2 min)
"To start — can you tell me a little about yourself and your
relationship with [problem area]?"
"How often do you [relevant activity]?"

PROBLEM DISCOVERY (8-10 min)
1. "What's the hardest part about [problem area] for you?"
   → Follow-up: "Can you tell me more about that?"

2. "Can you walk me through the last time you dealt with [problem]?"
   → Follow-up: "What happened next?"
   → Follow-up: "How did that make you feel?"

3. "Why was that hard / frustrating / time-consuming?"

4. "What have you tried to solve this? What tools/apps/services
   have you used?"
   → Follow-up: "How much time/money does that cost you?"
   → Follow-up: "What do you not like about those solutions?"

DEPTH QUESTIONS (3-5 min — pick 1-2 based on conversation)
- "What would happen if this problem were never solved?"
- "If you could wave a magic wand and fix this, what would change?"
- "What's the cost of NOT solving this? (time, money, stress)"
- "Who else in your life is affected by this problem?"

WRAP-UP (2 min)
"This has been really helpful. Is there anything I should have
asked about that I didn't?"

"Do you know 1-2 other people who deal with [problem] that
might be willing to chat with me?"

"Thanks so much — I really appreciate it."
```

### What NOT to Ask

**Avoid these question types entirely:**

| Bad Question | Why It's Bad | Better Alternative |
|-------------|-------------|-------------------|
| "Do you think this is a good idea?" | Invites compliments, not truth | "Tell me about the last time you dealt with [problem]" |
| "Would you buy a product that does X?" | Hypothetical; people lie about future behavior | "What have you spent money on to solve this?" |
| "How much would you pay for this?" | Stated WTP is unreliable | "What's the most expensive solution you've tried?" |
| "Don't you think it would be cool if..." | Leading question | "What solutions have you tried?" |
| "Do you ever..." | Invites generic/aspirational answers | "Tell me about a specific time when..." |
| "Would you ever..." | Hypothetical future | "When was the last time you..." |

- Source: [The Mom Test, via Khanna Law](https://www.khanna.law/notes/the-mom-test)
- Source: [Atlanta Ventures, "3 Rules from The Mom Test"](https://www.atlantaventures.com/blog/the-3-rules-to-customer-interviews-from-the-mom-test)

### Additional Useful Questions (from multiple sources)

**From The Mom Test:**
- "Why do you bother?" — identifies if the problem is truly painful
- "What are the implications of that?" — distinguishes annoying from costly
- "Talk me through what happened the last time..." — anchors to specific events
- "What else have you tried?" — reveals existing solutions and switching costs
- "Who else should I talk to?" — always end with this

**From Erik van der Pluijm's 100 Questions list:**
- "Can you tell me how you deal with [general problem space]?"
- "What do you think could be done to help you with [problem]?"
- "If you could wave a magic wand and instantly have any imaginable solution, what would it look like?"
- "Is there anything else you think I should know about that I didn't ask?"

- Source: [Erik van der Pluijm / WRKSHP, "100 Questions for Customer Development"](https://medium.com/wrkshp/100-questions-you-can-ask-in-customer-development-interviews-72945ad576f1)

**From Teresa Torres (story-based interviewing):**
- "Tell me about a time when..." — the foundational prompt for story-based interviews
- Don't ask who/what/why/how questions. Instead, collect specific stories about past real-world behavior. Opportunities emerge naturally from these stories.

- Source: [Product Talk / Teresa Torres, "Customer Interviews"](https://www.producttalk.org/customer-interviews/)
- Source: [Lenny's Newsletter interview with Teresa Torres](https://www.lennysnewsletter.com/p/teresa-torres-on-how-to-interview)

### The Commitment Test (end of interview)

Real validation comes from people giving you something of value. At the end of every interview, ask for at least one commitment:

**Time commitments:**
- Follow-up meeting with defined goals
- Feedback session on wireframes/prototype
- Agreeing to test the product when it exists

**Reputation commitments:**
- Introduction to someone else with the problem
- Introduction to a decision-maker (boss, spouse)
- Willing to be a reference or case study

**Financial commitments (strongest signal):**
- Pre-order or deposit
- Letter of intent
- Immediately pays for something

If someone says "great idea!" but won't introduce you to a friend who has the same problem, that's a red flag.

- Source: [The Mom Test, via Sachin Rekhi](https://www.sachinrekhi.com/the-mom-test-rob-fitzpatrick)

### Meeting Length Guidelines (from The Mom Test)

- **Early discovery (problem interviews):** 5-15 minutes is plenty
- **Solution validation (showing something):** ~30 minutes
- **Expert/industry interviews:** 60+ minutes

For student teams doing their first round: target 15-20 minutes. Shorter is better than longer — you'll do more interviews and get more diverse data.

- Source: [The Mom Test, via Rick Lindquist](https://www.ricklindquist.com/notes/the-mom-test-by-rob-fitzpatrick)

---

## 5. Synthesizing Interview Findings

### The Interview Snapshot (Teresa Torres)

After each interview, create a one-page summary that captures:
1. **Photo or sketch** of the participant (helps team remember who this was)
2. **Memorable quote** — the single sentence that struck you most
3. **Quick facts** — who they are, relevant context
4. **Experience map** — brief timeline of their experience with the problem
5. **Opportunities** — needs and pain points you identified
6. **Insights** — things you learned that you didn't know before

Teams can create these collaboratively in 15-20 minutes after each interview.

- Source: [Product Talk, "The Interview Snapshot"](https://www.producttalk.org/2024/02/interview-snapshot/)
- Templates available on: [Miro](https://miro.com/miroverse/interview-snapshot-template/), [Figma](https://www.figma.com/community/file/1118814957669913350/interview-snapshots-template)

### Affinity Mapping (for cross-interview synthesis)

After completing a batch of interviews, use affinity mapping to find patterns:

**Step 1: Prepare data**
- Transfer key observations, quotes, and insights onto individual sticky notes (physical or digital)
- One observation per note
- Source: [User Interviews, "Affinity Mapping"](https://www.userinterviews.com/blog/affinity-mapping-ux-research-data-synthesis)

**Step 2: Build initial clusters**
- Start with one note, then group related notes together
- Name each cluster as patterns emerge
- Don't force structure — let themes emerge organically

**Step 3: Iterate**
- Conduct multiple rounds
- Reorganize as new patterns surface
- "Multiple rounds allow you to recognize different patterns in the data"

**Step 4: Add contextual filters**
- Sort by demographic or behavioral segments
- Look for differences between user types

**Step 5: Analyze cross-cluster relationships**
- Zoom out — how do themes connect?
- Which themes are most common? Most painful? Most actionable?

- Source: [User Interviews, "Affinity Mapping"](https://www.userinterviews.com/blog/affinity-mapping-ux-research-data-synthesis)
- Tools: Miro, FigJam, or physical sticky notes on a wall

### Rapid Synthesis Spreadsheet

For student teams who want something simpler than full affinity mapping, a spreadsheet works:

**Column structure:**

| Interview # | Participant | Has Problem? | Problem Description (their words) | Severity (1-5) | Current Solution | $ Spent on Problem | Willing to Switch? | Top Quote | Referral? |
|---|---|---|---|---|---|---|---|---|---|

After filling in all interviews, look for:
- Most common problem descriptions (in their language)
- Average severity score
- Most common current solutions
- Quotes that could become landing page copy

- Adapted from: [LogRocket, "Managing Customer Discovery Data"](https://blog.logrocket.com/product-management/managing-customer-discovery-interview-data-spreadsheet-template/)
- Downloadable template: [Google Sheets template from LogRocket](https://docs.google.com/spreadsheets/d/1HWB3wYX3rWOHuzq9a4i1FiAuDcjE5FBZ-ZLSafxevcA/edit?usp=sharing)

### Decision Framework: What Do Your Interviews Tell You?

After synthesis, teams should answer these questions:

1. **Does the problem exist?** (Did people describe it unprompted?)
2. **How severe is it?** (Are people actively spending time/money to solve it?)
3. **Who has it worst?** (Which segment expressed the most pain?)
4. **What do they do today?** (What's the competitive landscape from the user's perspective?)
5. **Would they switch?** (Did they ask for a follow-up? Offer to test? Refer others?)
6. **What language do they use?** (Exact phrases for your landing page copy)

**Green light signals (proceed to vaporware):**
- 3+ people describe the same problem independently
- At least 1 person has spent money trying to solve it
- Multiple people asked "When can I try it?" or offered referrals
- You can articulate the problem in one sentence using their language

**Yellow light signals (modify and do more interviews):**
- People acknowledge the problem but say it's "not that bad"
- Everyone describes a different problem
- They have free/easy solutions that work "well enough"
- Interest but no commitments

**Red light signals (pivot):**
- Nobody recognizes the problem when you describe their own situation
- People solved it years ago and don't think about it
- "I wouldn't pay for that" + no alternative solutions being used
- Zero referrals or follow-up requests

### Team Debrief Format

After completing all interviews for the week:

**30-minute team debrief agenda:**

1. **Round-robin** (10 min): Each team member shares their #1 most surprising finding
2. **Pattern identification** (10 min): What did we hear more than once? What words/phrases repeated?
3. **Hypothesis check** (5 min): Does our original hypothesis hold? What would we change?
4. **Next steps** (5 min): What do we need for the vaporware site? What questions remain?

- Adapted from: [Product School, "Customer Discovery"](https://productschool.com/blog/user-experience/customer-discovery)

---

## 6. Interviews to Next Steps Pipeline

### How Interview Findings Feed Into Vaporware Sites

The direct connection between interviews and the Week 5 vaporware site:

**From interviews, extract:**
1. **The problem statement** — in the words your interviewees used (this becomes your headline)
2. **The pain points** — specific frustrations (these become your feature bullets)
3. **The current solutions** — what they're doing now (this becomes your "vs." comparison)
4. **The desired outcome** — what they wish existed (this becomes your value proposition)
5. **The objections** — what might prevent adoption (this informs your FAQ and trust signals)

**Mapping interviews to landing page elements:**

| Interview Finding | Landing Page Element |
|------------------|---------------------|
| Problem description (their words) | Headline / Hero text |
| Specific pain points | Feature bullets or problem cards |
| Desired outcome | Value proposition / subheadline |
| Current solution frustrations | "Why [Product] vs [Alternatives]" section |
| Most compelling quote | Social proof / testimonial |
| Target audience characteristics | Above-the-fold imagery and tone |
| Objections and concerns | FAQ section / trust signals |

- Source: [ABmatic, "Customer Pain Points for Landing Pages"](https://abmatic.ai/blog/how-to-use-customer-pain-points-to-target-saas-landing-page-messaging)
- Source: [Cursorup, "Pain Points to Boost Conversions"](https://www.cursorup.com/blog/pain-points)

### How Interview Findings Feed Into Wizard of Oz Tests

The Wizard of Oz test (Week 5, Thursday) uses interviews to:

1. **Define what to simulate** — Which features did interviewees say they needed most? Simulate those.
2. **Set realistic user expectations** — Based on how interviewees described their ideal solution, design the interface to match their mental model.
3. **Script the "wizard" responses** — Use the language and solutions interviewees described to craft realistic responses.
4. **Choose test participants** — Reach back out to interviewees who expressed the most interest. They're already primed.

**NN/g's Wizard of Oz process (5 steps):**
1. Create a prototype (can be Figma mockup)
2. Define responses (preset options vs. improvised vs. hybrid)
3. Develop a study protocol
4. Prepare the wizard (the team member operating behind the scenes)
5. Pilot test before real sessions

- Source: [NN/g, "The Wizard of Oz Method"](https://www.nngroup.com/articles/wizard-of-oz/)

### The Validation Chain: Interviews -> Vaporware -> Ads

```
Week 4: Interviews
  └── Extract: problem language, pain points, audience definition
       ↓
Week 5: Vaporware Site + Wizard of Oz
  └── Use interview language as copy
  └── Test with interview participants + new users
  └── Measure: do they engage? Sign up? Complete tasks?
       ↓
Week 6: Paid Ads to Vaporware
  └── Use winning interview phrases as ad copy
  └── Target audience segments identified in interviews
  └── Measure: CTR, signup rate, cost per signup
       ↓
Week 7: MVP Build
  └── Informed by everything above
  └── Build only what was validated
```

### Using Vaporware as a Validation Tool (Tomasz Tunguz)

**Sendwithus example:** Created a $28 experiment (landing page for a feature that didn't exist) to learn which features customers valued most. The data directly informed their product roadmap.

**Optimizely's founders:** Sold two $1,000 licenses based on a landing page before the product existed. That's the strongest form of pre-build validation — actual revenue.

**Key insight from Tunguz:** Vaporware as validation is "a terrific product discovery, product validation and demand generation technique" when used intentionally.

- Source: [Tomasz Tunguz, "Using Vaporware to Validate"](https://tomtunguz.com/competition-from-incumbents/)

---

## 7. Academic Research on Student Startup Validation

### Key Studies and Programs

**1. Stanford Lean LaunchPad (ENGR 245)**

The most documented university startup validation program. Taught by Steve Blank since 2011.

- Teams: 8-10 per cohort
- Weekly requirement: 10-15 customer interviews per team
- Total interviews: 900+ per cohort over the quarter
- Time commitment: 15-20 hours/week (double a normal class)
- 2024 result: All 8 teams formed companies (first time in 14 years)
- Each week: teams formulate 3-4 hypotheses, conduct ~10 interviews, converge on business ideas

- Source: [Steve Blank, "Lean LaunchPad Stanford 2024"](https://steveblank.com/2024/06/27/lean-launchpad-stanford-2024-8-teams-in-8-companies-out/)
- Source: [Stanford ENGR 245](https://leanlaunchpad.stanford.edu/)

**2. NSF I-Corps Program**

The national-scale adoption of Lean LaunchPad methodology.

- Structure: 7-week accelerated program
- Team composition: PI + entrepreneurial lead + industry mentor
- Requirement: minimum 100 face-to-face interviews
- Curriculum: Lean Startup + Business Model Canvas hypothesis testing
- Faculty reported "positive impressions" and "specific knowledge gains"
- Students test hypotheses about customer needs and validate or invalidate through direct conversation

- Source: [NSF I-Corps About](https://www.nsf.gov/funding/initiatives/i-corps/about-i-corps)
- Source: [VentureWell, "Lean LaunchPad Educators Guide"](https://venturewell.org/wp-content/uploads/Educators-Guide-Final-w-cover-PDF.pdf)

**3. Penn State Customer Discovery 101**

Free, self-guided program specifically designed for university entrepreneurs.

- Includes downloadable workbook with exercises and templates
- Covers: identifying target markets, designing experiments, analyzing insights
- Designed for students without access to traditional business support
- No Penn State affiliation required

- Source: [Invent Penn State, "Customer Discovery 101"](https://invent.psu.edu/programs/cd101/)
- Workbook: [PDF download](https://customerdiscovery101.launchbox.psu.edu/files/Customer%20Discovery%20101%20Workbook%203-21-24.pdf)

**4. Shepherd & Gruber (2021) — Academic Framework**

"The Lean Startup Framework: Closing the Academic-Practitioner Divide" — major academic paper analyzing the five building blocks of lean startup methodology:
1. Business model
2. Validated learning / customer development
3. Minimum viable product
4. Perseverance vs. pivoting
5. Market-opportunity navigation

Key finding: The paper identifies a significant gap between how practitioners use lean startup and what academic research has validated. Calls for more rigorous study of these methods.

- Source: [Shepherd & Gruber (2021), Entrepreneurship Theory and Practice](https://journals.sagepub.com/doi/10.1177/1042258719899415)

**5. Self-Regulated Learning in Lean Startup Education**

Study examining how individual learning (self-regulated) and team learning interact in lean startup courses.

- Context: Project-based course where students validated real entrepreneurial problems
- Finding: Self-regulated learning is proposed as an effective way for individual students to learn in entrepreneurial projects
- At the team level: Team learning and psychological safety contribute to group performance
- Published in: Technological Forecasting and Social Change

- Source: [ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0040162515000281)

**6. Lean LaunchPad as Qualitative Research**

NSF-published paper arguing that customer discovery as practiced in Lean LaunchPad is a legitimate form of qualitative research methodology.

- Source: [NSF PAGES, "Lean LaunchPad and Customer Discovery as Qualitative Research"](https://par.nsf.gov/biblio/10072084-lean-launchpad-customer-discovery-form-qualitative-research)

**7. University Incubator Systematic Review (2018-2024)**

Comprehensive systematic review using PRISMA protocol, covering 200 articles on university incubators.

- Finding: Global university venturing has seen 25% increase since 2021
- Practical + theoretical courses together yield better results than theory alone
- Multi-stakeholder validation is an important characteristic of academic entrepreneurship
- Institutions providing incubators/accelerators offer "not only physical resources but also mentorship, social capital, and a platform for iterative learning"

- Source: [MDPI Sustainability, "Academic Entrepreneurship Evolution"](https://www.mdpi.com/2071-1050/17/12/5365)

**8. Student Entrepreneurship Evolution (ScienceDirect, 2023)**

- Identifies the trend from classroom-centered education to experiential learning
- Students exposed to real-life entrepreneurship contexts show different outcomes than traditional instruction
- Emphasizes the importance of structured support mechanisms

- Source: [ScienceDirect, "The Evolution of Student Entrepreneurship"](https://www.sciencedirect.com/science/article/pii/S1472811723000587)

### Key Findings Across Academic Literature

**What works in university startup validation programs:**

1. **Mandatory interview quotas** — Programs like I-Corps and Lean LaunchPad that require specific interview counts produce better outcomes than unstructured "go talk to people" assignments.

2. **Weekly presentation cadence** — Stanford's Lean LaunchPad requires teams to present their findings every week. This accountability structure prevents procrastination and forces synthesis.

3. **Hypothesis-driven approach** — Framing interviews as "hypothesis tests" rather than "conversations" makes the learning more structured and actionable.

4. **Team-based interviewing** — Having interview pairs (one asks, one notes) produces better data and shared understanding.

5. **Experiential learning** — Combination of practical and theoretical produces better results than theory alone. "The greatest value of university incubators is their capacity to teach life skills."

**What doesn't work:**

1. **Unsupervised interview assignments** — Without structure, students default to asking friends and family, leading to the Friends & Family Fallacy.

2. **Surveys before interviews** — Students gravitate toward surveys (easier, less scary) but miss the depth of interviews.

3. **Skipping the problem stage** — Engineering-minded students especially want to jump to building.

### Mixed Evidence on Lean Startup in Education

It's worth noting: "Concerning outcomes associated with the use of the lean startup, the literature is not equivocal. This finding is due to the diverse methods, populations, endpoints, and business sectors." The evidence is mixed, with some studies showing significant benefits and others showing limited impact.

However, the research consistently shows that rigorous implementation — with coaching, accountability, and structure — produces better outcomes than casual adoption.

- Source: [Hilaris Publisher, "Lean Startup Limitations and Outcomes"](https://www.hilarispublisher.com/open-access/lean-startup-as-an-entrepreneurial-strategy-limitations-outcomes-and-learnings-for-practitioners.pdf)

---

## Key Resources for Students

### Books / Required Reading
- **The Mom Test** by Rob Fitzpatrick — the single best resource on customer interviews
- **Sprint** by Jake Knapp — prototype chapter for Week 5 vaporware
- **Continuous Discovery Habits** by Teresa Torres — interview snapshot method

### Free Online Resources
- [Customer Dev Labs Script Generator](https://customerdevlabs.com/script/) — generates a custom 5-question interview script
- [Penn State Customer Discovery 101 Workbook](https://customerdiscovery101.launchbox.psu.edu/files/Customer%20Discovery%20101%20Workbook%203-21-24.pdf) — free downloadable workbook
- [Kromatic Note-Taking Template](https://www.kromatic.com/innovation-resources/customer-discovery-note-taking-template) — printable PDF for in-person interviews
- [Kromatic Note-Taking Template (Google Doc)](https://www.kromatic.com/innovation-resources/customer-discovery-note-taking-template-google-doc) — for remote calls
- [Kromatic Conversation Insights Template](https://kromatic.com/innovation-resources/Customer-Conversation-Insights-Template) — for organizing insights
- [Interview Snapshot Template (Miro)](https://miro.com/miroverse/interview-snapshot-template/) — Teresa Torres's one-page synthesis format
- [Interview Snapshot Template (Figma)](https://www.figma.com/community/file/1118814957669913350/interview-snapshots-template)

### Useful Articles
- [Rick Lindquist, "Notes from The Mom Test"](https://www.ricklindquist.com/notes/the-mom-test-by-rob-fitzpatrick) — best free summary
- [Khanna Law, "The Mom Test"](https://www.khanna.law/notes/the-mom-test) — comprehensive reference with all questions
- [Customer Dev Labs, "How I Interview Customers"](https://customerdevlabs.com/2013/11/05/how-i-interview-customers/) — Justin Wilcox's framework
- [Product Talk, "Customer Interviews"](https://www.producttalk.org/customer-interviews/) — Teresa Torres's approach
- [Product Talk, "Finding Interview Participants Before Customers"](https://www.producttalk.org/finding-interview-participants-before-customers/) — practical recruitment guide
- [NNGroup, "How Many Participants for a UX Interview?"](https://www.nngroup.com/articles/interview-sample-size/) — sample size guidance
- [NNGroup, "The Wizard of Oz Method in UX"](https://www.nngroup.com/articles/wizard-of-oz/) — for Week 5 preparation

---

*End of research notes. These are raw research findings with citations — not a final report.*
*Complements the broader technique taxonomy in `research_notes_pre_build_validation.md`.*
